---
title: "Practical Machine Learning Course Project: Qualitative Activity Recognition"
author: "Tom Tegtmeyer"
date: "April 6, 2016"
output: 
  html_document: 
    keep_md: yes
---

# 1: Synopsis
In this assignment, we use a random forest algorithm to create a model that will predict how an exercise was performed based upon various accelerometer measurements. As [described](http://groupware.les.inf.puc-rio.br/har) by the authors, 

> Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

After reducing our training set to 52 predictor variables, we create the random forest model, which has an encouraging 0.29% estimated out-of-bag error rate. Finally, we use our model to predict the classes for a 20-subject test set.


# 2: Downloading and Processing the Data

First, we load the packages we will be using for the analysis.
```{r,message=FALSE,warning=FALSE}
library(caret); library(randomForest); library(scatterplot3d)
```


Next, we load the training and test sets.
```{r, cache=TRUE}
training<-read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
testing<-read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
```

Before creating the prediction model, we need to clean up the dataset. We remove the near zero variance variables. Eliminating the X variable is essential, because it is merely the index value, and the test set has its own indexing. We also remove the user_name, time stamp, and window variables. (We could leave the time stamp and numwindow variables in, but it almost feels like cheating to include them. Indeed, when included, part 1 of the raw time stamp and window number are the most important variables.) Finally, we remove the variables with mostly NA values. As a result, all of the variables used in our model come from actual measurements.

```{r, cache=TRUE}
#eliminating the near zero variance variables
nsv<-nearZeroVar(training,saveMetrics = TRUE) 
training<-training[,nsv$nzv==FALSE]
#eliminating the index, username, timestamp, and num_window variables 
training<-training[,-(1:6)]
#eliminating the variables that have mostly NA values
sums<-colSums(is.na(training))
nacols<-as.vector(sums>2)
training<-training[,nacols==FALSE]
```

#3: Creating the model

Now that we have processed the data, we will create a random forest prediction model based on the training data, using the randomForest package. (Note: we are not using train() from the caret package because of performance issues related to the size of the dataset.)

```{r,cache=TRUE}
set.seed(125)
training.rf<-randomForest(classe~.,data=training)
training.rf
```

The dataset seems to be particulary amenable to the random forest approach. The confusion matrix reveals a relatively small number of misclassified values in the training set, and the estimated out-of-bag error is a tiny 0.29%. Note that, according to the inventors of the random forest algorithm, [Breiman and Cutler](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr), "In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error." Despite that, for educational purposes, we will run our own cross-validation in the next section.  

Next, we create a plot of the 10 most important variables in the model.

```{r}
varImpPlot(training.rf, n.var=10, main = "Variable Importance")
```

The following is a 3-dimensional scatterplot of the three most important variables, colored by the "classe" variable. We see a bit of separation in the different groups.

```{r}
with(training,scatterplot3d(roll_belt,yaw_belt,pitch_forearm,color=as.numeric(classe),pch=19,main="Top 3 variables, colored by classe"))
legend("topright",col=1:6,c("A","B","C","D","E"),pch=19)
```

# 4: Cross-Validation

As mentioned above, to estimate the out of sample error, we perform k-fold cross-validation (with k = 10) on the training set with the random forest algorithm.

```{r,cache=TRUE}
set.seed(12321)
#create folds for cross validation
folds<-createFolds(y=training$classe,k=10,list=TRUE,returnTrain=TRUE)
acc<-numeric(10)
for (i in 1:10){
        cv.train<-training[folds[[i]],] #create training set for fold
        cv.test<-training[-folds[[i]],] #create testing set
        cvrf<-randomForest(classe~.,data=cv.train)
        cvpred<-predict(cvrf,cv.test)
        #extract accuracy rate
        acc[i]<-confusionMatrix(cvpred,cv.test$classe)$overall[1]
        print(paste("Accuracy for fold", i, ":", acc[i]))
        rm(cvrf)
}
```

```{r}
noquote(paste("Mean accuracy rate: ",mean(acc),sep=""))
noquote(paste("Mean error rate: ", round((1-mean(acc))*100,2),"%",sep=""))
```

We see that the mean error rate is close to the out-of-bag error estimate of 0.29% from the randomForest output.

# 5: The Prediction
Applying our random forest model to the test set gives us the following predictions for the "classe"" variable.

```{r}
pred<-predict(training.rf,testing)
pred
```

It is informative to see the "vote" breakdown for each subject in the test set.

```{r}
pred2<-predict(training.rf,testing,"vote")
pred2
apply(pred2,1,max) #find each row maximum
```

This final line of output gives the vote totals for the predicted classes. Note that the minimum of these is 0.718 for subject 3. We can be fairly confident of all of our predictions.
